{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\n\ndef mkdir_if_not_exist(dirPath):\n    dirPath=dirPath.strip()\n    if os.path.exists(dirPath):\n        shutil.rmtree(dirPath) \n    os.makedirs(dirPath)\n    \n# folder path\ndata_folder_path = '../input/backblaze-2018-data-clean-1'\nresult_folder_path = './data'\n\nselected_columns = ['time_interval','date','model','capacity_bytes','failure',\n                    'smart_1_normalized',\n#                     'smart_1_raw', # Smart 1: Raw_Read_Error_Rate (Raw Value)\n                    'smart_3_normalized',\n#                     'smart_3_raw', # Smart 3: Spin_Up_Time (Raw Value)\n                    'smart_5_normalized',\n#                     'smart_5_raw', # Smart 5: Reallocated_Sector_Ct (Raw Value)\n                    'smart_7_normalized',\n#                     'smart_7_raw', # Smart 7: Seek_Error_Rate (Raw Value)\n                    'smart_9_normalized',\n#                     'smart_9_raw', # Smart 9: Power_On_Hours (Raw Value)\n                    'smart_187_normalized',\n#                     'smart_187_raw', # Smart 187: Reported_Uncorrect (Raw Value)\n#                     'smart_188_normalized','smart_188_raw',\n#                     'smart_189_normalized','smart_189_raw', # Smart 189: High_Fly_Writes (Raw Value)\n#                     'smart_193_normalized','smart_193_raw',\n#                     'smart_194_normalized','smart_194_raw', # Smart 194: Temperature_Celsius (Raw Value)\n#                     'smart_195_normalized','smart_195_raw', # Smart 195: Hardware_ECC_Recovered (Raw Value)\n#                     'smart_197_normalized','smart_197_raw', # Smart 197: Current_Pending_Sector (Raw Value)\n#                     'smart_198_normalized','smart_198_raw',\n#                     'smart_241_normalized','smart_241_raw',\n#                     'smart_242_normalized','smart_242_raw'\n                    ]\n\n# 读取所有的文件\nmkdir_if_not_exist(result_folder_path)\nFiles = os.listdir(data_folder_path)\nfor f in Files:\n    fPath = data_folder_path+'/'+f\n    if os.path.isdir(fPath):\n        dataFiles = os.listdir(fPath) \n    for file in dataFiles:\n        filePath = fPath + '/' + file\n        all_df = pd.read_csv(filePath)\n        model_type =  all_df.loc[0]['model']\n        if model_type == 'ST4000DM000': # 指定硬盘类型\n            filtered_df = all_df.loc[:,selected_columns]\n            clean_csv_file = result_folder_path + '/' + file\n            with open(clean_csv_file, 'w+') as f:\n                filtered_df.to_csv(f)\n                \nshutil.make_archive(\"filtered_data\", 'zip', result_folder_path)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T02:00:19.045489Z","iopub.execute_input":"2022-01-12T02:00:19.046175Z","iopub.status.idle":"2022-01-12T02:01:09.748540Z","shell.execute_reply.started":"2022-01-12T02:00:19.046082Z","shell.execute_reply":"2022-01-12T02:01:09.747835Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\n# 将时间序列输出为带标签的监督学习样本\ndef series_to_supervised(features, target, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(features) is list else features.shape[1]\n    df = pd.DataFrame(features)\n    flag = pd.DataFrame(target)\n    cols, names = [], []\n    # 添加过去的时间序列变量 (var1(t-7) - var6(t-7))->(var6(t-1)-var6(t-1))\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i)) # axis = 0 is set by default, row向下移动i个单位\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # 添加未来需预测数据 var1(t)-var6(t)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    cols.append(flag)\n    names += ['failure']\n    agg = pd.concat(cols, axis=1) # shape = [rows, 6*（7+1）+ 1 = 49]\n    agg.columns = names\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg\n\n# n_vars feature numbers\ndef prepare_data(filepath, n_in, n_out=1, n_vars=6):\n    dataset = pd.read_csv(dataPath).fillna(0).iloc[:,6:] # only features columns\n    target = pd.read_csv(dataPath).fillna(0).iloc[:,5] # failure feature\n    feature_values = dataset.values.astype('float32')\n    target = target.values.astype('float32')\n    reframed = series_to_supervised(feature_values, target, n_in, n_out)\n    contain_vars = []\n    contain_vars += [('var%d(t)' % (j+1)) for j in range(n_vars)] # var1(t) -> var6(t)\n    for i in range(1, n_in+1): # i = 1-7\n        # (var1(t-1)->var6(t-1))->(var1(t-1)->var6(t-7))\n        contain_vars += [('var%d(t-%d)' % (j, i)) for j in range(1,n_vars+1)] \n#     data = reframed [ contain_vars + ['var1(t)'] + [('var1(t+%d)' % (j)) for j in range(1,n_out)]]\n    # (var1(t) -> var6(t)) | var1(t-1)->var6(t-1))->(var1(t-1)->var6(t-7) | failure\n    data = reframed [contain_vars + ['failure']]\n    #修改列名\n    col_names = ['Y(t)','X1','X2','X3','X4','X5','X6']\n    contain_vars = []\n    contain_vars += [('Y(t)var%d' % (j+1)) for j in range(n_vars)] # Y(t)(var1->var6)\n    for j in range(1,n_in+1):\n        # (X1(t-1)->X6(t-1))\n        contain_vars += [('%s(t-%d)' % (col_names[i], j)) for i in range(1,n_vars+1)] \n#     data.columns = contain_vars +  ['Y(t)'] + [('Y(t+%d)' % (j)) for j in range(1,n_out)]\n    data.columns = contain_vars + ['target']\n    return data\n\n# 划分数据集\ndef train_test_split(data, n_vars=6, train_proportion=0.8):\n    values = data.values\n    n_train = round(data.shape[0]*train_proportion)\n    train = values[:n_train, :]\n    test = values[n_train:, :]\n    #分隔输入X和输出y\n    train_X, train_y = train[:, :(n_in+1)*n_vars], train[:, (n_in+1)*n_vars:]\n    test_X, test_y = test[:, :(n_in+1)*n_vars], test[:, (n_in+1)*n_vars:]\n    #将输入X改造为LSTM的输入格式，即[samples,timesteps,features]\n    train_X = train_X.reshape((train_X.shape[0], n_in+1, n_vars))\n    test_X = test_X.reshape((test_X.shape[0], n_in+1, n_vars))\n    return train_X, train_y, test_X, test_y\n\ndef fit_lstm(data_splited, n_neurons=50, n_batch=72, n_epoch=100, loss='mae', optimizer='adam', repeats=1):\n    train_X, train_y, test_X, test_y = data_splited\n    model_list = []\n    for i in range(repeats):\n        #设计神经网络\n        model = Sequential()\n        model.add(LSTM(n_neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n        model.add(Dense(train_y.shape[1]))\n        model.compile(loss=loss, optimizer=optimizer)\n        #拟合神经网络\n        history = model.fit(train_X, train_y, epochs=n_epoch, batch_size=n_batch, validation_data=(test_X, test_y), verbose=0, shuffle=False)\n        #画出学习过程\n        p1 = pyplot.plot(history.history['loss'], color='blue', label='train')\n        p2 = pyplot.plot(history.history['val_loss'], color='yellow',label='test')\n        #保存model\n        model_list.append(model)\n    pyplot.legend([\"train\",\"test\"])\n    pyplot.show()\n    return model_list\n\ndef lstm_predict(model, data_prepare):\n    scaler = data_prepare[0]\n    test_X = data_prepare[4]\n    test_y = data_prepare[5]\n    #做出预测\n    yhat = model.predict(test_X)\n    #将测试集上的预测值还原为原来的数据维度\n    scale_new = MinMaxScaler()\n    scale_new.min_, scale_new.scale_ = scaler.min_[0], scaler.scale_[0]\n    inv_yhat = scale_new.inverse_transform(yhat)\n    #将测试集上的实际值还原为原来的数据维度\n    inv_y = scale_new.inverse_transform(test_y)\n    return inv_yhat, inv_y\n\nn_in = 7 # (t-1) -> (t-7)\nn_out = 1 # target(t)\nn_neuron = 5 \nn_batch = 16\nn_epoch = 200\nrepeats = 5\n\ndataFiles = os.listdir(result_folder_path)\ndataFiles = dataFiles[:]\nframes = []\nfor data in dataFiles:\n    dataPath = result_folder_path+'/'+data\n    data_prepared = prepare_data(dataPath, n_in, n_out)\n    frames.append(data_prepared)\n#     print(data + \">>>>>>>\" + str(data_prepared.shape))\ndata = pd.concat(frames) # \ndata_splited = train_test_split(data)\nmodel_list = fit_lstm(data_splited, n_neuron, n_batch, n_epoch,repeats=repeats)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-12T02:01:13.851745Z","iopub.execute_input":"2022-01-12T02:01:13.852213Z"},"trusted":true},"execution_count":null,"outputs":[]}]}